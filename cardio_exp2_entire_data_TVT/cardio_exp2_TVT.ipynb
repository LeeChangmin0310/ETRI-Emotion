{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7b9478",
   "metadata": {},
   "source": [
    "# Description: \n",
    "##### This script is designed for a pre-split train/test data structure.\n",
    "##### For each of the 6 conditions, it performs the following steps, ensuring no data leakage:\n",
    "1. Loads the specific train and test CSV files.\n",
    "2. Initializes PyCaret, ~~which learns Z-score normalization parameters ONLY from the train data and applies them to both train and test sets.~~\n",
    "3. Runs 5-fold cross-validation on the normalized train data to find the best model.\n",
    "4. Evaluates the best model on the unseen, normalized test data.\n",
    "5. Saves the final test performance results to the './res' directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da93039",
   "metadata": {},
   "source": [
    "# Instructions:\n",
    "1. Make sure you have PyCaret and its dependencies installed.\n",
    "If not, uncomment the line below and run it once.\n",
    "> !pip install pycaret pandas\n",
    "\n",
    "2. Place this script in the same directory as your 90 CSV data files.\n",
    "\n",
    "3. Run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e18e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2746f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leechangmin/Desktop/Project/ETRI-Emotion/cardio_exp2_entire_data_TVT\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee8689c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_with_pre_split_normalized_data():\n",
    "    \"\"\"\n",
    "    Runs a 5-fold CV workflow on pre-split and normalized data.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    TARGET_COLUMN_NAME = 'label'\n",
    "    DATA_DIR = './data'\n",
    "    RESULT_DIR = './res'\n",
    "    # Adjusted group names to match filenames\n",
    "    GROUPS = ['All', 'High', 'Low']\n",
    "    VARIABLES = ['arousal', 'valence']\n",
    "    \n",
    "    # Create the result directory if it doesn't exist\n",
    "    os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"=====================================================================\")\n",
    "    print(\"===  STARTING EXPERIMENT: CV on Pre-Split, Normalized Data  ===\")\n",
    "    print(\"=====================================================================\")\n",
    "\n",
    "    for group in GROUPS:\n",
    "        for variable in VARIABLES:\n",
    "            experiment_name = f\"{group}_{variable}\"\n",
    "            train_file_path = os.path.join(DATA_DIR, 'train', f\"{experiment_name}_train.csv\")\n",
    "            test_file_path = os.path.join(DATA_DIR, 'test', f\"{experiment_name}_test.csv\")\n",
    "            \n",
    "            if not (os.path.exists(train_file_path) and os.path.exists(test_file_path)):\n",
    "                print(f\"\\n--- Skipping {experiment_name}: Train or test file not found. ---\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n--- Processing: {experiment_name} ---\")\n",
    "            \n",
    "            # 1. Load the pre-split train and test datasets\n",
    "            train_df = pd.read_csv(train_file_path)\n",
    "            test_df = pd.read_csv(test_file_path)\n",
    "            \n",
    "            # Rename the actual target column ('arousal' or 'valence') to 'label'\n",
    "            # so PyCaret can find it. The `variable` holds the correct column name for each loop.\n",
    "            train_df.rename(columns={variable: TARGET_COLUMN_NAME}, inplace=True)\n",
    "            test_df.rename(columns={variable: TARGET_COLUMN_NAME}, inplace=True)\n",
    "            \n",
    "            print(f\"  > Renamed column '{variable}' to '{TARGET_COLUMN_NAME}'.\")\n",
    "            \n",
    "            # 2. Setup PyCaret environment.\n",
    "            # Pass train_df and test_df directly.\n",
    "            # Enable Z-score normalization. PyCaret handles this correctly to prevent data leakage.\n",
    "            s = setup(data=train_df,\n",
    "                      test_data=test_df,\n",
    "                      target=TARGET_COLUMN_NAME,\n",
    "                      fold=5,\n",
    "                      \n",
    "                      # Enable Z-score normalization\n",
    "                      normalize=True,\n",
    "                      normalize_method='zscore',\n",
    "                      \n",
    "                      index=False,\n",
    "                      session_id=123,\n",
    "                      verbose=False)\n",
    "            \n",
    "            # 3. Compare models using 5-fold CV on the normalized training data\n",
    "            print(\"  > Comparing models using 5-fold CV on the normalized training set...\")\n",
    "            best_model = compare_models(verbose=False)\n",
    "            \n",
    "            # 4. Finalize the best model (retrains on the entire normalized training set)\n",
    "            print(f\"  > Finalizing the best model: {pull().iloc[0,0]}\")\n",
    "            final_model = finalize_model(best_model)\n",
    "            \n",
    "            # 5. Evaluate the final model on the unseen, normalized test set\n",
    "            print(\"  > Evaluating the final model on the hold-out test set...\")\n",
    "            # predict_model() without a data argument uses the test_data provided during setup\n",
    "            predict_model(final_model, verbose=False)\n",
    "            \n",
    "            # 6. Pull the performance metrics from the test set evaluation\n",
    "            test_metrics = pull()\n",
    "            \n",
    "            # 7. Save the test set performance to a CSV file\n",
    "            output_path = os.path.join(RESULT_DIR, f\"test_performance_{experiment_name}.csv\")\n",
    "            test_metrics.to_csv(output_path)\n",
    "            print(f\"  > Test performance for '{pull().iloc[0,0]}' saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf772b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "===  STARTING EXPERIMENT: CV on Pre-Split, Normalized Data  ===\n",
      "=====================================================================\n",
      "\n",
      "--- Processing: All_arousal ---\n",
      "  > Renamed column 'arousal' to 'label'.\n",
      "  > Comparing models using 5-fold CV on the normalized training set...\n",
      "  > Finalizing the best model: Dummy Classifier\n",
      "  > Evaluating the final model on the hold-out test set...\n",
      "  > Test performance for 'Dummy Classifier' saved to ./res/test_performance_All_arousal.csv\n",
      "\n",
      "--- Processing: All_valence ---\n",
      "  > Renamed column 'valence' to 'label'.\n",
      "  > Comparing models using 5-fold CV on the normalized training set...\n",
      "  > Finalizing the best model: Naive Bayes\n",
      "  > Evaluating the final model on the hold-out test set...\n",
      "  > Test performance for 'Naive Bayes' saved to ./res/test_performance_All_valence.csv\n",
      "\n",
      "--- Processing: High_arousal ---\n",
      "  > Renamed column 'arousal' to 'label'.\n",
      "  > Comparing models using 5-fold CV on the normalized training set...\n",
      "  > Finalizing the best model: Dummy Classifier\n",
      "  > Evaluating the final model on the hold-out test set...\n",
      "  > Test performance for 'Dummy Classifier' saved to ./res/test_performance_High_arousal.csv\n",
      "\n",
      "--- Processing: High_valence ---\n",
      "  > Renamed column 'valence' to 'label'.\n",
      "  > Comparing models using 5-fold CV on the normalized training set...\n",
      "  > Finalizing the best model: Linear Discriminant Analysis\n",
      "  > Evaluating the final model on the hold-out test set...\n",
      "  > Test performance for 'Linear Discriminant Analysis' saved to ./res/test_performance_High_valence.csv\n",
      "\n",
      "--- Processing: Low_arousal ---\n",
      "  > Renamed column 'arousal' to 'label'.\n",
      "  > Comparing models using 5-fold CV on the normalized training set...\n",
      "  > Finalizing the best model: Dummy Classifier\n",
      "  > Evaluating the final model on the hold-out test set...\n",
      "  > Test performance for 'Dummy Classifier' saved to ./res/test_performance_Low_arousal.csv\n",
      "\n",
      "--- Processing: Low_valence ---\n",
      "  > Renamed column 'valence' to 'label'.\n",
      "  > Comparing models using 5-fold CV on the normalized training set...\n",
      "  > Finalizing the best model: Naive Bayes\n",
      "  > Evaluating the final model on the hold-out test set...\n",
      "  > Test performance for 'Naive Bayes' saved to ./res/test_performance_Low_valence.csv\n",
      "\n",
      "\n",
      "All experiments are complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_cv_with_pre_split_normalized_data()\n",
    "    print(\"\\n\\nAll experiments are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f2a1ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "===  STARTING EXPERIMENT: Evaluating Top 5 Models on Test Set  ===\n",
      "=====================================================================\n",
      "\n",
      "--- Processing: All_arousal ---\n",
      "  > Comparing models to find the top 5...\n",
      "  > Evaluating each of the top 5 models on the test set...\n",
      "  > Top 5 model test performances saved to ./res2/top5_test_performance_All_arousal.csv\n",
      "\n",
      "--- Processing: All_valence ---\n",
      "  > Comparing models to find the top 5...\n",
      "  > Evaluating each of the top 5 models on the test set...\n",
      "  > Top 5 model test performances saved to ./res2/top5_test_performance_All_valence.csv\n",
      "\n",
      "--- Processing: High_arousal ---\n",
      "  > Comparing models to find the top 5...\n",
      "  > Evaluating each of the top 5 models on the test set...\n",
      "  > Top 5 model test performances saved to ./res2/top5_test_performance_High_arousal.csv\n",
      "\n",
      "--- Processing: High_valence ---\n",
      "  > Comparing models to find the top 5...\n",
      "  > Evaluating each of the top 5 models on the test set...\n",
      "  > Top 5 model test performances saved to ./res2/top5_test_performance_High_valence.csv\n",
      "\n",
      "--- Processing: Low_arousal ---\n",
      "  > Comparing models to find the top 5...\n",
      "  > Evaluating each of the top 5 models on the test set...\n",
      "  > Top 5 model test performances saved to ./res2/top5_test_performance_Low_arousal.csv\n",
      "\n",
      "--- Processing: Low_valence ---\n",
      "  > Comparing models to find the top 5...\n",
      "  > Evaluating each of the top 5 models on the test set...\n",
      "  > Top 5 model test performances saved to ./res2/top5_test_performance_Low_valence.csv\n",
      "\n",
      "\n",
      "All experiments are complete.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Description:\n",
    "# This script identifies the top 5 models based on 5-fold cross-validation on the\n",
    "# training set. It then finalizes each of these 5 models and evaluates their\n",
    "# performance on the unseen test set, saving the combined results.\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "from pycaret.classification import *\n",
    "import os\n",
    "\n",
    "def evaluate_top_5_models_on_test_set():\n",
    "    \"\"\"\n",
    "    Finds the top 5 models via CV and evaluates each on the test set.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    TARGET_COLUMN_NAME = 'label'\n",
    "    DATA_DIR = './data'\n",
    "    RESULT_DIR = './res2'\n",
    "    GROUPS = ['All', 'High', 'Low']\n",
    "    VARIABLES = ['arousal', 'valence']\n",
    "    \n",
    "    os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"=====================================================================\")\n",
    "    print(\"===  STARTING EXPERIMENT: Evaluating Top 5 Models on Test Set  ===\")\n",
    "    print(\"=====================================================================\")\n",
    "\n",
    "    for group in GROUPS:\n",
    "        for variable in VARIABLES:\n",
    "            experiment_name = f\"{group}_{variable}\"\n",
    "            train_file_path = os.path.join(DATA_DIR, 'train', f\"{experiment_name}_train.csv\")\n",
    "            test_file_path = os.path.join(DATA_DIR, 'test', f\"{experiment_name}_test.csv\")\n",
    "            \n",
    "            if not (os.path.exists(train_file_path) and os.path.exists(test_file_path)):\n",
    "                print(f\"\\n--- Skipping {experiment_name}: Train or test file not found. ---\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n--- Processing: {experiment_name} ---\")\n",
    "            \n",
    "            try:\n",
    "                # 1. Load data and rename target column\n",
    "                train_df = pd.read_csv(train_file_path)\n",
    "                test_df = pd.read_csv(test_file_path)\n",
    "                train_df.rename(columns={variable: TARGET_COLUMN_NAME}, inplace=True)\n",
    "                test_df.rename(columns={variable: TARGET_COLUMN_NAME}, inplace=True)\n",
    "                \n",
    "                # 2. Setup PyCaret environment\n",
    "                s = setup(data=train_df,\n",
    "                          target=TARGET_COLUMN_NAME,\n",
    "                          fold=5,\n",
    "                          index=False,\n",
    "                          session_id=123,\n",
    "                          verbose=False)\n",
    "                \n",
    "                # 3. Compare models and get the top 5\n",
    "                print(\"  > Comparing models to find the top 5...\")\n",
    "                # n_select=5 returns a list of the 5 best model objects\n",
    "                top_5_models = compare_models(n_select=5, verbose=False)\n",
    "                \n",
    "                # This list will hold the performance DataFrame for each of the top 5 models\n",
    "                top_5_test_results = []\n",
    "\n",
    "                print(\"  > Evaluating each of the top 5 models on the test set...\")\n",
    "                for model in top_5_models:\n",
    "                    # 4. Finalize each model (retrain on full training data)\n",
    "                    final_model = finalize_model(model)\n",
    "                    \n",
    "                    # 5. Evaluate on the test set\n",
    "                    predict_model(final_model, data=test_df, verbose=False)\n",
    "                    \n",
    "                    # 6. Pull the performance metrics and add to our list\n",
    "                    test_metrics = pull()\n",
    "                    top_5_test_results.append(test_metrics)\n",
    "                \n",
    "                # 7. Combine the results of the 5 models into a single DataFrame\n",
    "                final_results_df = pd.concat(top_5_test_results).reset_index(drop=True)\n",
    "                \n",
    "                # 8. Save the combined results to a CSV file\n",
    "                output_path = os.path.join(RESULT_DIR, f\"top5_test_performance_{experiment_name}.csv\")\n",
    "                final_results_df.to_csv(output_path, index=False)\n",
    "                print(f\"  > Top 5 model test performances saved to {output_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  > An error occurred during processing: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_top_5_models_on_test_set()\n",
    "    print(\"\\n\\nAll experiments are complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da597e6",
   "metadata": {},
   "source": [
    "### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e157c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas display options to prevent truncation\n",
    "# Set options to display all rows and columns without truncation\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000) # Adjust width to prevent line wrapping\n",
    "\n",
    "def load_all_results():\n",
    "    \"\"\"\n",
    "    Read 'results_{group}_{variable}_manual_5fold_summary.csv' formatted files\n",
    "    and return a dictionary of DataFrames.\n",
    "    \"\"\"\n",
    "    # --- Configuration (can be adjusted as needed) ---\n",
    "    groups = ['All', 'High', 'Low']\n",
    "    variables = ['arousal', 'valence']\n",
    "    \n",
    "    # dictionary to hold all results\n",
    "    all_results = {}\n",
    "    \n",
    "    print(\"Reading all result CSV files...\")\n",
    "\n",
    "    # Iterate through each group and variable to construct filenames\n",
    "    for group in groups:\n",
    "        for variable in variables:\n",
    "            # Construct the filename based on the group and variable\n",
    "            experiment_name = f\"{group}_{variable}\"\n",
    "            filename = f\"./res/test_performance_{experiment_name}.csv\"\n",
    "            \n",
    "            try:\n",
    "                # CSV file into DataFrame\n",
    "                # index_col=0 to use the first column as index\n",
    "                df = pd.read_csv(filename, index_col='Model') # Use 'Model' column as index\n",
    "                \n",
    "                # Store the DataFrame in the dictionary with the experiment name as key\n",
    "                all_results[experiment_name] = df\n",
    "                print(f\"  - Successfully loaded: {filename}\")\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(f\"  - File not found, skipping: {filename}\")\n",
    "                \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08ee35c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading all result CSV files...\n",
      "  - Successfully loaded: ./res/test_performance_All_arousal.csv\n",
      "  - Successfully loaded: ./res/test_performance_All_valence.csv\n",
      "  - Successfully loaded: ./res/test_performance_High_arousal.csv\n",
      "  - Successfully loaded: ./res/test_performance_High_valence.csv\n",
      "  - Successfully loaded: ./res/test_performance_Low_arousal.csv\n",
      "  - Successfully loaded: ./res/test_performance_Low_valence.csv\n",
      "\n",
      "==================================================\n",
      "      <<< Summary of All Loaded Results >>>\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Performance Results for [All_arousal] ---\n",
      "                  Unnamed: 0  Accuracy  AUC  Recall   Prec.     F1  Kappa  MCC\n",
      "Model                                                                         \n",
      "Dummy Classifier           0    0.5625  0.5  0.5625  0.3164  0.405    0.0  0.0\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "--- Performance Results for [All_valence] ---\n",
      "             Unnamed: 0  Accuracy     AUC  Recall   Prec.      F1  Kappa  MCC\n",
      "Model                                                                        \n",
      "Naive Bayes           0    0.4375  0.6257  0.4375  0.1914  0.2663    0.0  0.0\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "--- Performance Results for [High_arousal] ---\n",
      "                  Unnamed: 0  Accuracy  AUC  Recall   Prec.      F1  Kappa  MCC\n",
      "Model                                                                          \n",
      "Dummy Classifier           0     0.375  0.5   0.375  0.1406  0.2045    0.0  0.0\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "--- Performance Results for [High_valence] ---\n",
      "                              Unnamed: 0  Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "Model                                                                                             \n",
      "Linear Discriminant Analysis           0    0.5833  0.6875  0.5833  0.5833  0.5833  0.1667  0.1667\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "--- Performance Results for [Low_arousal] ---\n",
      "                  Unnamed: 0  Accuracy  AUC  Recall   Prec.      F1  Kappa  MCC\n",
      "Model                                                                          \n",
      "Dummy Classifier           0      0.75  0.5    0.75  0.5625  0.6429    0.0  0.0\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "--- Performance Results for [Low_valence] ---\n",
      "             Unnamed: 0  Accuracy     AUC  Recall   Prec.      F1  Kappa  MCC\n",
      "Model                                                                        \n",
      "Naive Bayes           0     0.375  0.7685   0.375  0.1406  0.2045    0.0  0.0\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Run the function to load all result data\n",
    "    loaded_results = load_all_results()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"      <<< Summary of All Loaded Results >>>\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if not loaded_results:\n",
    "        print(\"No result files were found.\")\n",
    "    else:\n",
    "        # Loop through and print each result DataFrame\n",
    "        for name, result_df in loaded_results.items():\n",
    "            print(f\"\\n\\n--- Performance Results for [{name}] ---\")\n",
    "            # Using print() on a DataFrame with the options set will display it fully\n",
    "            print(result_df)\n",
    "            print(\"-\"*(len(name) + 35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5efa7889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading all result CSV files...\n",
      "  - Successfully loaded: ./res2/top5_test_performance_All_arousal.csv\n",
      "  - Successfully loaded: ./res2/top5_test_performance_All_valence.csv\n",
      "  - Successfully loaded: ./res2/top5_test_performance_High_arousal.csv\n",
      "  - Successfully loaded: ./res2/top5_test_performance_High_valence.csv\n",
      "  - Successfully loaded: ./res2/top5_test_performance_Low_arousal.csv\n",
      "  - Successfully loaded: ./res2/top5_test_performance_Low_valence.csv\n",
      "\n",
      "==================================================\n",
      "      <<< Summary of All Loaded Results (5 models per experiment) >>>\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Performance Results for [All_arousal] ---\n",
      "                                 Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "Model                                                                                    \n",
      "Extra Trees Classifier             0.5521  0.4590  0.5521  0.5263  0.4993  0.0255  0.0309\n",
      "Random Forest Classifier           0.4479  0.3902  0.4479  0.3992  0.4085 -0.1811 -0.1999\n",
      "CatBoost Classifier                0.5000  0.4572  0.5000  0.4631  0.4609 -0.0726 -0.0812\n",
      "Gradient Boosting Classifier       0.5208  0.4572  0.5208  0.4649  0.4504 -0.0514 -0.0669\n",
      "Light Gradient Boosting Machine    0.5000  0.4048  0.5000  0.4779  0.4787 -0.0549 -0.0577\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "--- Performance Results for [All_valence] ---\n",
      "                                 Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "Model                                                                                    \n",
      "Extra Trees Classifier             0.5417  0.5192  0.5417  0.5370  0.5384  0.0588  0.0590\n",
      "Light Gradient Boosting Machine    0.5417  0.5604  0.5417  0.5417  0.5417  0.0688  0.0688\n",
      "Extreme Gradient Boosting          0.5833  0.5450  0.5833  0.5812  0.5820  0.1489  0.1491\n",
      "CatBoost Classifier                0.6042  0.5776  0.6042  0.6090  0.6056  0.2042  0.2049\n",
      "Random Forest Classifier           0.6146  0.5743  0.6146  0.6119  0.6127  0.2107  0.2111\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "--- Performance Results for [High_arousal] ---\n",
      "                          Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "Model                                                                             \n",
      "Extra Trees Classifier      0.4167  0.4454  0.4167  0.6136  0.3159  0.0345  0.0778\n",
      "Ada Boost Classifier        0.3750  0.4537  0.3750  0.4408  0.3304 -0.0909 -0.1325\n",
      "Naive Bayes                 0.3958  0.5870  0.3958  0.7686  0.2480  0.0252  0.1130\n",
      "Random Forest Classifier    0.4167  0.4611  0.4167  0.5595  0.3389  0.0175  0.0325\n",
      "Dummy Classifier            0.3750  0.5000  0.3750  0.1406  0.2045  0.0000  0.0000\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "--- Performance Results for [High_valence] ---\n",
      "                                 Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "Model                                                                                    \n",
      "Extra Trees Classifier             0.4583  0.5052  0.4583  0.4531  0.4429 -0.0833 -0.0884\n",
      "CatBoost Classifier                0.4583  0.4670  0.4583  0.4556  0.4497 -0.0833 -0.0861\n",
      "Gradient Boosting Classifier       0.4167  0.4844  0.4167  0.4111  0.4074 -0.1667 -0.1721\n",
      "Random Forest Classifier           0.5000  0.4627  0.5000  0.5000  0.4965  0.0000  0.0000\n",
      "Light Gradient Boosting Machine    0.4792  0.5000  0.4792  0.4772  0.4678 -0.0417 -0.0436\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "--- Performance Results for [Low_arousal] ---\n",
      "                           Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "Model                                                                              \n",
      "Random Forest Classifier     0.5833  0.5289  0.5833  0.5833  0.5833 -0.1111 -0.1111\n",
      "Extra Trees Classifier       0.5833  0.6238  0.5833  0.5833  0.5833 -0.1111 -0.1111\n",
      "Ada Boost Classifier         0.5833  0.6019  0.5833  0.5833  0.5833 -0.1111 -0.1111\n",
      "CatBoost Classifier          0.6042  0.6481  0.6042  0.5928  0.5983 -0.0857 -0.0859\n",
      "Extreme Gradient Boosting    0.5625  0.6181  0.5625  0.5955  0.5773 -0.0769 -0.0778\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "--- Performance Results for [Low_valence] ---\n",
      "                                 Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
      "Model                                                                                    \n",
      "Extra Trees Classifier             0.6250  0.6574  0.6250  0.6562  0.6310  0.2500  0.2582\n",
      "Random Forest Classifier           0.6042  0.6009  0.6042  0.6087  0.6062  0.1648  0.1650\n",
      "CatBoost Classifier                0.6667  0.6000  0.6667  0.6602  0.6622  0.2727  0.2739\n",
      "Gradient Boosting Classifier       0.5833  0.3852  0.5833  0.5651  0.5703  0.0698  0.0710\n",
      "Light Gradient Boosting Machine    0.5208  0.5185  0.5208  0.5154  0.5179 -0.0337 -0.0337\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Pandas display options to prevent truncation\n",
    "# Set options to display all rows and columns without truncation\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000) # Adjust width to prevent line wrapping\n",
    "\n",
    "def load_all_results():\n",
    "    \"\"\"\n",
    "    Read 'results_{group}_{variable}_manual_5fold_summary.csv' formatted files\n",
    "    and return a dictionary of DataFrames.\n",
    "    \"\"\"\n",
    "    # --- Configuration (can be adjusted as needed) ---\n",
    "    groups = ['All', 'High', 'Low']\n",
    "    variables = ['arousal', 'valence']\n",
    "    \n",
    "    # dictionary to hold all results\n",
    "    all_results = {}\n",
    "    \n",
    "    print(\"Reading all result CSV files...\")\n",
    "\n",
    "    # Iterate through each group and variable to construct filenames\n",
    "    for group in groups:\n",
    "        for variable in variables:\n",
    "            # Construct the filename based on the group and variable\n",
    "            experiment_name = f\"{group}_{variable}\"\n",
    "            filename = f\"./res2/top5_test_performance_{experiment_name}.csv\"\n",
    "            \n",
    "            try:\n",
    "                # CSV file into DataFrame\n",
    "                # index_col=0 to use the first column as index\n",
    "                df = pd.read_csv(filename, index_col='Model') # Use 'Model' column as index\n",
    "                \n",
    "                # Store the DataFrame in the dictionary with the experiment name as key\n",
    "                all_results[experiment_name] = df\n",
    "                print(f\"  - Successfully loaded: {filename}\")\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(f\"  - File not found, skipping: {filename}\")\n",
    "                \n",
    "    return all_results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run the function to load all result data\n",
    "    loaded_results = load_all_results()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"      <<< Summary of All Loaded Results (5 models per experiment) >>>\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if not loaded_results:\n",
    "        print(\"No result files were found.\")\n",
    "    else:\n",
    "        # Loop through and print each result DataFrame\n",
    "        for name, result_df in loaded_results.items():\n",
    "            print(f\"\\n\\n--- Performance Results for [{name}] ---\")\n",
    "            # Using print() on a DataFrame with the options set will display it fully\n",
    "            print(result_df)\n",
    "            print(\"-\"*(len(name) + 35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0160e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etri-emotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
